{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load single graph data\n",
    "이는 단일 구조물의 graph data로, dictionary와 유사한 구조로 접근한다.\n",
    "\n",
    "- x: node features\n",
    "- edge_index: node pairs\n",
    "- edge_attr: edge features\n",
    "- Acceleration, Velocity, Desplacement, Moment_Z, Shear_Y, 원시 반응 시간이력 데이터의 크기는 모두 [2000, 8]，반응 시간이력의 샘플링 간격 0.05，최대 길이를 100초로 설정했으므로 2000 스텝이 됨. 8은 예측 가능한 최대 건물 층수.\n",
    "- ground_motion (mm/$s^2$): [time_steps]\n",
    "- time_steps: 전처리되지 않은 ground motion의 스텝 수\n",
    "- sample_rate: 지표 가속도 데이터의 취득 주기(반응 시간이력 샘플링 주기의 10배)\n",
    "- ground_motion_name: 지진 기록명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[147, 15], edge_index=[2, 636], edge_attr=[636, 6], Acceleration=[2000, 8], Velocity=[2000, 8], Displacement=[2000, 8], Moment_Z=[147, 2000, 6], Shear_Y=[147, 2000, 6], ground_motion=[6000], time_steps=6000, sample_rate=0.005, ground_motion_name='1999.09.21_01.47.159_Chichi_TCU011_A900_90.0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "sample_path = \"./Data/Linear_Analysis/eval/ChiChi/structure_401/structure_graph.pt\"\n",
    "sample_graph = torch.load(sample_path, weights_only=False)\n",
    "print(sample_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data with GraphDataset\n",
    "로드할 데이터셋 경로와 반응 종류를 입력한다.\n",
    "\n",
    "GraphDataset은 1차 전처리를 수행해 구조 반응 시간이력 y를 3차원 벡터로 변환하여 LSTM 모델이 사용하기 쉽게 하고, 해당 구조물의 건물 층수와 반응 종류를 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ./Data/Linear_Analysis/eval/ChiChi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:04<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ./Data/Linear_Analysis/eval/NGAWest2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32;49m ==================================================================================================== \u001b[0;0m\n",
      "\u001b[0;32;49m  number of effective data: 10 \u001b[0;0m\n",
      "\u001b[0;32;49m ==================================================================================================== \u001b[0;0m\n",
      "Data(x=[147, 15], edge_index=[2, 636], edge_attr=[636, 6], ground_motion=[1, 20000], time_steps=6000, sample_rate=0.005, ground_motion_name='1999.09.21_01.47.159_Chichi_TCU011_A900_90.0', response_type='Velocity', story=6, y=[1, 2000, 8])\n",
      "\n",
      " # of data:10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from Utils.dataset import GraphDataset\n",
    "\n",
    "ChiCHi_folder_dir = \"./Data/Linear_Analysis/eval/ChiChi\"\n",
    "NGAWest2_folder_dir = \"./Data/Linear_Analysis/eval/NGAWest2\"\n",
    "\n",
    "sampleDataset = GraphDataset(folder_path_list = [ChiCHi_folder_dir, NGAWest2_folder_dir], response_type = \"Velocity\", numOfData_per_folder = 5)\n",
    "\n",
    "print(sampleDataset[0])\n",
    "print(f\"\\n # of data:{len(sampleDataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization dictionary\n",
    "데이터를 모델에 입력하기 전에 정규화를 거쳐 각 feature 차원을 [-1, 1]로 조정해야 하므로, 각 feature 차원의 최대값이 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(source) normalization state: False\n",
      "\n",
      "(target) normalization state: False\n",
      "\n",
      "normalization dictionary: \n",
      " {'x': {'XYZ_gridline_num': tensor(8.), 'XYZ_grid_index': tensor(7.), 'period': tensor(1.2711), 'DOF': tensor(1.), 'mass': tensor(0.0252), 'XYZ_inertia': tensor(255288.), 'XYZ_mode_shape': tensor(2.1000)}, 'ground_motion': tensor(6140.5513), 'y': tensor(2678.), 'edge_attr': {'S_y': tensor(3687090.), 'S_z': tensor(3687090.), 'area': tensor(30774.), 'element_length': tensor(8000.)}, 'response_type': 'Velocity'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"(source) normalization state: {sampleDataset.source_norm_state}\")\n",
    "print(f\"\\n(target) normalization state: {sampleDataset.target_norm_state}\")\n",
    "normalized_item_dict = sampleDataset.get_normalized_item_dict()\n",
    "print(\"\\nnormalization dictionary: \\n\", normalized_item_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noramalize\n",
    "각 feature 차원의 최대값을 사용하여 데이터를 정규화한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(source) normalization state: True\n",
      "\n",
      "(target) normalization state: True\n"
     ]
    }
   ],
   "source": [
    "sampleDataset.normalize_source(normalized_item_dict)\n",
    "sampleDataset.normalize_target(normalized_item_dict)\n",
    "print(f\"(source) normalization state: {sampleDataset.source_norm_state}\")\n",
    "print(f\"\\n(target) normalization state: {sampleDataset.target_norm_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denormalize\n",
    "모델 계산이 끝난 후 계산 결과를 원래 스케일로 역정규화한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(source) normalization state: False\n",
      "\n",
      "(target) normalization state: False\n"
     ]
    }
   ],
   "source": [
    "sampleDataset.denormalize_source(normalized_item_dict)\n",
    "sampleDataset.denormalize_target(normalized_item_dict)\n",
    "print(f\"(source) normalization state: {sampleDataset.source_norm_state}\")\n",
    "print(f\"\\n(target) normalization state: {sampleDataset.target_norm_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader (PyG)\n",
    "DataLoader를 반복자로 사용하며, 학습 시에는 batch size를 자유롭게 조정하고 shuffle=True로 설정한다. 예측 시에는 batch size=1로 하고 shuffle=False로 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======\n",
      "mini batch 0:\n",
      "DataBatch(x=[147, 15], edge_index=[2, 636], edge_attr=[636, 6], ground_motion=[1, 20000], time_steps=[1], sample_rate=[1], ground_motion_name=[1], response_type=[1], story=[1], y=[1, 2000, 8], batch=[147], ptr=[2])\n",
      "\n",
      "=======\n",
      "mini batch 1:\n",
      "DataBatch(x=[210, 15], edge_index=[2, 904], edge_attr=[904, 6], ground_motion=[1, 20000], time_steps=[1], sample_rate=[1], ground_motion_name=[1], response_type=[1], story=[1], y=[1, 2000, 8], batch=[210], ptr=[2])\n",
      "\n",
      "=======\n",
      "mini batch 2:\n",
      "DataBatch(x=[168, 15], edge_index=[2, 742], edge_attr=[742, 6], ground_motion=[1, 20000], time_steps=[1], sample_rate=[1], ground_motion_name=[1], response_type=[1], story=[1], y=[1, 2000, 8], batch=[168], ptr=[2])\n",
      "\n",
      "=======\n",
      "mini batch 3:\n",
      "DataBatch(x=[105, 15], edge_index=[2, 444], edge_attr=[444, 6], ground_motion=[1, 20000], time_steps=[1], sample_rate=[1], ground_motion_name=[1], response_type=[1], story=[1], y=[1, 2000, 8], batch=[105], ptr=[2])\n",
      "\n",
      "=======\n",
      "mini batch 4:\n",
      "DataBatch(x=[245, 15], edge_index=[2, 1116], edge_attr=[1116, 6], ground_motion=[1, 20000], time_steps=[1], sample_rate=[1], ground_motion_name=[1], response_type=[1], story=[1], y=[1, 2000, 8], batch=[245], ptr=[2])\n",
      "\n",
      "=======\n",
      "mini batch 5:\n",
      "DataBatch(x=[168, 15], edge_index=[2, 742], edge_attr=[742, 6], ground_motion=[1, 20000], time_steps=[1], sample_rate=[1], ground_motion_name=[1], response_type=[1], story=[1], y=[1, 2000, 8], batch=[168], ptr=[2])\n",
      "\n",
      "=======\n",
      "mini batch 6:\n",
      "DataBatch(x=[336, 15], edge_index=[2, 1582], edge_attr=[1582, 6], ground_motion=[1, 20000], time_steps=[1], sample_rate=[1], ground_motion_name=[1], response_type=[1], story=[1], y=[1, 2000, 8], batch=[336], ptr=[2])\n",
      "\n",
      "=======\n",
      "mini batch 7:\n",
      "DataBatch(x=[192, 15], edge_index=[2, 868], edge_attr=[868, 6], ground_motion=[1, 20000], time_steps=[1], sample_rate=[1], ground_motion_name=[1], response_type=[1], story=[1], y=[1, 2000, 8], batch=[192], ptr=[2])\n",
      "\n",
      "=======\n",
      "mini batch 8:\n",
      "DataBatch(x=[144, 15], edge_index=[2, 630], edge_attr=[630, 6], ground_motion=[1, 20000], time_steps=[1], sample_rate=[1], ground_motion_name=[1], response_type=[1], story=[1], y=[1, 2000, 8], batch=[144], ptr=[2])\n",
      "\n",
      "=======\n",
      "mini batch 9:\n",
      "DataBatch(x=[63, 15], edge_index=[2, 252], edge_attr=[252, 6], ground_motion=[1, 20000], time_steps=[1], sample_rate=[1], ground_motion_name=[1], response_type=[1], story=[1], y=[1, 2000, 8], batch=[63], ptr=[2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader as GraphDataLoader\n",
    "exclude_keys=[\"sample_rate\", \"ground_motion_name\", \"response_type\"]\n",
    "sample_loader = GraphDataLoader(sampleDataset, batch_size=1, shuffle=False, )\n",
    "\n",
    "for (i, data) in enumerate(sample_loader):\n",
    "    print('=======')\n",
    "    print(f'mini batch {i}:' )\n",
    "    # data = data.to(\"cuda\")\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pack and Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======\n",
      "mini-batch:\n",
      "DataBatch(x=[147, 15], edge_index=[2, 636], edge_attr=[636, 6], ground_motion=[1, 20000], time_steps=[1], sample_rate=[1], ground_motion_name=[1], response_type=[1], story=[1], y=[1, 2000, 8], batch=[147], ptr=[2])\n",
      "\n",
      "=======\n",
      "size of compressed ground motion:\n",
      "torch.Size([1, 1000, 20])\n",
      "\n",
      "PPS length list:\n",
      "tensor([300.])\n",
      "\n",
      "=======\n",
      "size of unpacked ground motion:\n",
      "torch.Size([1, 1000, 20])\n",
      "\n",
      "check x has unpacked to original format\n",
      "True\n",
      "\n",
      "unpack length list:\n",
      "tensor([300])\n"
     ]
    }
   ],
   "source": [
    "compression_rate = 20\n",
    "data = next(iter(sample_loader))\n",
    "ground_motion = data.ground_motion.reshape(-1, int(data.ground_motion.size(1)/compression_rate), compression_rate)\n",
    "ground_motion = ground_motion.to(\"cuda\")\n",
    "\n",
    "packed_length_list = data.time_steps / compression_rate\n",
    "total_length = ground_motion.size(1)\n",
    "print('=======')\n",
    "print(\"mini-batch:\")\n",
    "print(data)\n",
    "print()\n",
    "\n",
    "print('=======')\n",
    "print(\"size of compressed ground motion:\")\n",
    "print(ground_motion.size())\n",
    "print()\n",
    "print(\"PPS length list:\")\n",
    "print(packed_length_list)\n",
    "print()\n",
    "\n",
    "packed_x = nn.utils.rnn.pack_padded_sequence(ground_motion, lengths = packed_length_list, batch_first=True, enforce_sorted=False)\n",
    "seq_unpacked_x, lens_unpacked = nn.utils.rnn.pad_packed_sequence(packed_x, batch_first=True, total_length = total_length)\n",
    "\n",
    "print('=======')\n",
    "print(\"size of unpacked ground motion:\")\n",
    "print(seq_unpacked_x.size())\n",
    "print()\n",
    "print(\"check x has unpacked to original format\")\n",
    "print(torch.equal(seq_unpacked_x, ground_motion))\n",
    "print()\n",
    "print(\"unpack length list:\")\n",
    "print(lens_unpacked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myproject2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
